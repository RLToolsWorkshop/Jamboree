{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file by version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get files\n",
    "How we want to get the files greatly determines the accessing schemes.\n",
    "\n",
    "## Accessing Schemes\n",
    "\n",
    "### By Query Key\n",
    "\n",
    "Here we want to find a model by query key. If our query is:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"entity\": \"storage\",\n",
    "    \"category\": \"model\",\n",
    "    \"subcategory\": {\"info\": \"...\", \"goes\": \"...\", \"here\": \"...\"},\n",
    "    \"name\": \"name goes here\"\n",
    "}\n",
    "```\n",
    "\n",
    "We would want to seek to find such a model inside of our data-store. This is the most basic of uses.\n",
    "\n",
    "### By Version\n",
    "\n",
    "We want to be able to find our models by version as well. That's so we can play a game of version control over all of our models. So for example, if we want to find all models with the above key, and say we want version `1.0.4`, we could immediately find that version.\n",
    "\n",
    "### By Checksum\n",
    "\n",
    "We want to be able to find models by checksum as well. That's so we can delete all related information later and update models. To prevent the same file from being stored to multiple query keys we'll append both the query key and version number to the checksum information.\n",
    "\n",
    "## Required fields for all files:\n",
    "\n",
    "1. **Global checksum set** - here we'll store all the existing checksums\n",
    "2. **Checksum information** - here we'll store all relavent information for the given checksum. Information includes:\n",
    "    * File size\n",
    "    * Query Key\n",
    "    * Latest Version Number\n",
    "3. **Sorted version set** - Here we'll store all existing versions inside of a sorted set. We can get the latest version for any given query key by simply getting the latest version.\n",
    "    * We could end up running a global sorting algorithm by finding the highest ranking.\n",
    "    * We could create a score per version if it comes to that\n",
    "4. **Save file by `querykey:version`** - here we'll actually store the model. The reason we'll run this type of query is so we can store the file only in a single place. If we're running a replace operation by checksum we can also run a checksum lookup (if the checksum exist) and simply create a key that will replace the given file/blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/kevin/.pyenv/versions/3.7.3/lib/python3.7/site-packages/arctic/_util.py:6: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n",
      "/home/kevin/.pyenv/versions/3.7.3/lib/python3.7/site-packages/arctic/store/_pandas_ndarray_store.py:6: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import DataFrame, Series, Panel\n"
     ]
    }
   ],
   "source": [
    "from redis import Redis\n",
    "from jamboree.utils.core import consistent_hash\n",
    "from jamboree.utils.support import serialize, deserialize, create_checksum\n",
    "from jamboree.utils.core import consistent_hash, consistent_unhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import hmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "redconnection = Redis()\n",
    "test_query_key = {\n",
    "    \"entity\": \"storage\",\n",
    "    \"category\": \"model\",\n",
    "    \"subcategory\": {\n",
    "        \"info\": \"...\", \n",
    "        \"goes\": \"...\", \n",
    "        \"here\": \"...\"\n",
    "    },\n",
    "    \"name\": \"name_goes_here\"\n",
    "}\n",
    "def to_ser(func):\n",
    "    sblob = serialize(func)\n",
    "    csum = create_checksum(sblob)\n",
    "    return sblob, csum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Function and file information\n",
    "example_function = lambda x: x + 1\n",
    "ser, csum = to_ser(example_function)\n",
    "json_key_hash = consistent_hash(test_query_key)\n",
    "init_version = \"0.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Checksum list\n",
    "\n",
    "We're going to start with all of the basic operations. Then actually get to the real meat of the global checksum list. That would be using it in everyday operations. \n",
    "\n",
    "* Saving the checksum inside of a global set\n",
    "* Storing a serialized file to a query key and version\n",
    "* creating a hash function and serializing the file to be used for both above\n",
    "\n",
    "After we do that we'll be doing the following in order:\n",
    "\n",
    "1. Checking to see if we've already stored a file inside of the database\n",
    "2. Getting the latest version by file\n",
    "3. Preventing storage again if the file already exist\n",
    "4. Removing existing files\n",
    "5. Correcting the information about the given file (version and query information) by checksum (if it exist)\n",
    "6. Allowing overwritting a file if we allow it\n",
    "7. Try loading a file that's not inside of our checksum set and make sure it doesn't load\n",
    "8. Updating the version number if we already have a version number\n",
    "9. Getting the latest version inside of a given list of versions for a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checksum(s_sum:str, q_hash:str, f_version:str):\n",
    "    \"\"\" Gets checksum information and saves it to a global table \"\"\"\n",
    "    checksumlistname = \"globalchecksumset\"\n",
    "    current_checksum_key = f\"{s_sum}:sum_info\"\n",
    "    redconnection.sadd(checksumlistname, s_sum)\n",
    "    sum_info = {\n",
    "        \"version\": f_version,\n",
    "        \"query_key\": q_hash\n",
    "    }\n",
    "    redconnection.hmset(current_checksum_key, sum_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_store(item, q_hash, version):\n",
    "    \"\"\"Stores a file by query_hash and version number\"\"\"\n",
    "    qkey = f\"{q_hash}:{version}\"\n",
    "    redconnection.set(qkey, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_blob(query, item):\n",
    "    \"\"\" Carries out all operations pertaining saving a single item \"\"\"\n",
    "    ser, csum = to_ser(item)\n",
    "    json_key_hash = consistent_hash(query)\n",
    "    init_version = \"0.0.1\"\n",
    "    save_checksum(csum, json_key_hash, init_version)\n",
    "    file_store(ser, json_key_hash, init_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blob(test_query_key, example_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Operations\n",
    "We'll use these operations to determine if something already exist. Some other things to consider:\n",
    "\n",
    "1. Is there a version set that's not empty?\n",
    "2. Is there a checksum that has associated information included?\n",
    "3. Is the latest version information correct? Does the information from the checksum match up?\n",
    "\n",
    "\n",
    "We might want to separate everything about the data here into a set of classes:\n",
    "\n",
    "1. SuperStorage - The class that everything will be inside. Checksums, version control, syncing data at varying levels.\n",
    "1. Versioning - Controls everything about the version control. Such as getting the latest version, checking if a version exist for a given query key, etc.\n",
    "2. Checksums - A way of determining everything there is about the checksum. \n",
    "3. Synchronization - A way of tying everything together using coherent logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_checksum(csum):\n",
    "    checksumlistname = \"globalchecksumset\"\n",
    "    global_exist = bool(redconnection.exists(checksumlistname))\n",
    "    if global_exist is False:\n",
    "        print(\"Global Checklist Doesn't Exist\")\n",
    "        return False\n",
    "    redconnection.sadd(checksumlistname, csum)\n",
    "    file_exist = bool(redconnection.sismember(checksumlistname, csum))\n",
    "    if file_exist is False:\n",
    "        print(\"The file doesn't exist inside of the system\")\n",
    "        return False\n",
    "    \n",
    "    current_checksum_key = f\"{checksumlistname}:sum_info\"\n",
    "    file_info_exist = bool(redconnection.exists(current_checksum_key))\n",
    "    if file_info_exist is False:\n",
    "        print(\"The file info doesn't exist inside of the system\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file info doesn't exist inside of the system\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_checksum(csum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import maya\n",
    "from redis.client import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import dill\n",
    "import lz4.frame\n",
    "import version_query\n",
    "from version_query import VersionComponent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisFileProcessor(object):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._pipe = None\n",
    "        self._conn = None\n",
    "    \n",
    "    @property\n",
    "    def conn(self) -> Redis:\n",
    "        if self._conn is None:\n",
    "            raise AttributeError(\"Pipe hasn't been set\")\n",
    "        return self._conn\n",
    "    @conn.setter\n",
    "    def conn(self, _pipe:Redis):\n",
    "        self._conn = _pipe\n",
    "    \n",
    "    @property\n",
    "    def pipe(self) -> Pipeline:\n",
    "        if self._pipe is None:\n",
    "            raise AttributeError(\"Pipe hasn't been set\")\n",
    "        return self._pipe\n",
    "    @pipe.setter\n",
    "    def pipe(self, _pipe:Pipeline):\n",
    "        self._pipe = _pipe\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.pipe = None\n",
    "        self.conn = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisVersioning(RedisFileProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._current_checksum = None\n",
    "        self._query_hash = None\n",
    "        self._current_version = None\n",
    "        self.is_update = False\n",
    "        \n",
    "    def index(self):\n",
    "        \"\"\"Get the current index of the versions\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def vkey(self):\n",
    "        version_key = f\"{self.qhash}:version_set\"\n",
    "        return version_key\n",
    "    \n",
    "    @property\n",
    "    def version(self):\n",
    "        if self._current_version is None:\n",
    "            self._current_version = self.latest\n",
    "        return self._current_version\n",
    "    \n",
    "    @property\n",
    "    def qhash(self):\n",
    "        if self._query_hash is None:\n",
    "            raise AttributeError(\"Checksum hasn't been loaded yet\")\n",
    "        return self._query_hash\n",
    "    \n",
    "    @qhash.setter\n",
    "    def qhash(self, qhash:str):\n",
    "        self._query_hash = qhash\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def exist(self) -> bool:\n",
    "        is_exist = bool(self.conn.exists(self.vkey))\n",
    "        version_count = self.conn.zcard(self.vkey)\n",
    "        above_zero = (version_count >= 0)\n",
    "        if is_exist and above_zero:\n",
    "            self.is_update = True\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def latest(self):\n",
    "        \"\"\"Get the latest version given a hash\"\"\"\n",
    "        if self.exist is False:\n",
    "            self.conn.zadd(self.vkey, {\"0.0.1\": maya.now()._epoch})\n",
    "            return \"0.0.1\"\n",
    "        return (self.conn.zrange(self.vkey, -1, -1)[0]).decode()\n",
    "        \n",
    "    @property\n",
    "    def updated(self):\n",
    "        current = self.version\n",
    "        if self.is_update is False:\n",
    "            return current\n",
    "        vs = version_query.Version.from_str(current)\n",
    "        \n",
    "        new_vs = vs.increment(VersionComponent.Patch)\n",
    "        new_vs_str = new_vs.to_str()\n",
    "        \n",
    "        self.conn.zadd(self.vkey, {new_vs_str: maya.now()._epoch})\n",
    "        return new_vs_str\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.is_update = False\n",
    "        self._current_version = None\n",
    "        print(\"Resetting versioning variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class RedisChecksum(RedisFileProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._current_checksum = None\n",
    "        self.master = \"f35e91e082494a278d64221230cbaae7:master\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def checksum(self):\n",
    "        if self._current_checksum is None:\n",
    "            raise AttributeError(\"Checksum hasn't been loaded yet\")\n",
    "        return self._current_checksum\n",
    "    \n",
    "    @checksum.setter\n",
    "    def checksum(self, _checksum:str):\n",
    "        self._current_checksum = _checksum\n",
    "    \n",
    "    @property\n",
    "    def incheck(self):\n",
    "        return f\"{self.checksum}:sum_info\"\n",
    "    \n",
    "    @property\n",
    "    def exists(self) -> bool:\n",
    "        \"\"\"Does the checksum exist? \"\"\"\n",
    "#         self.watch_all()\n",
    "        \n",
    "        global_exist = bool(self.conn.exists(self.master))\n",
    "        file_exist = bool(self.conn.sismember(self.master, self.checksum))\n",
    "        file_info_exist = bool(self.conn.exists(self.incheck))\n",
    "        # Check to see if the global set exist\n",
    "        if global_exist is False:\n",
    "            print(\"Global checksum store doesn't exist\")\n",
    "            \n",
    "        if file_exist is False:\n",
    "            print(\"File doesn't exist\")\n",
    "            return False\n",
    "        \n",
    "        if file_info_exist is False:\n",
    "            print(\"File info doesn't exist\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def watch_all(self):\n",
    "        \"\"\"Watch all of the given variables \"\"\"\n",
    "        self.pipe.watch(self.master)\n",
    "        self.pipe.watch(self.incheck)\n",
    "        \n",
    "    @property\n",
    "    def info(self):\n",
    "#         self.watch_all()\n",
    "        if not self.exists:\n",
    "            raise AttributeError(\"Information given the checksum doesn't exist\")\n",
    "        \n",
    "        _info = self.conn.hmgetall(self.incheck)\n",
    "        return _info\n",
    "    \n",
    "    def add(self, version:str, qkey:str):\n",
    "#         self.watch_all()\n",
    "        \"\"\"\n",
    "            Add the checksum into the database with the given query key.\n",
    "            \n",
    "            We're not dynamically adding version information into the database\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            \"version\": version,\n",
    "            \"query_key\": qkey\n",
    "        }\n",
    "        self.conn.hmset(self.incheck, info)\n",
    "        self.conn.sadd(self.master, self.checksum)\n",
    "        print(self.conn.smembers(self.master))\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"\n",
    "            Remove everything related to the checksum from the database\n",
    "        \"\"\"\n",
    "        if self.exists:\n",
    "            self.conn.delete(self.incheck)\n",
    "            self.conn.srem(self.master, self.checksum)            \n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class RedisSynchronizer(RedisFileProcessor):\n",
    "    \"\"\"\n",
    "        Runs all of the logic about the given object:\n",
    "        \n",
    "        1. Appropiately saves information for the file and related to the file\n",
    "        2. Deletes all related information for a given file \n",
    "        3. Checks for consistency between bits of information\n",
    "        4. Has rules to save information related to the given bit of data\n",
    "            * If we're going to just replace the current version\n",
    "            * If we're going to increment the given version\n",
    "            * Has deletion rules as well (deleting large swabs of information)\n",
    "    \"\"\"\n",
    "    \n",
    "    def add(self, qhash, version, pickled_file):\n",
    "        vkey = f\"{qhash}:{version}\"\n",
    "        self.conn.set(vkey, pickled_file)\n",
    "    \n",
    "    def load(self, qhash:str, version:str):\n",
    "        vkey = f\"{qhash}:{version}\"\n",
    "        is_exist = bool(self.conn.exists(vkey))\n",
    "        if is_exist == False:\n",
    "            raise AttributeError(\"File doesn't exist\")\n",
    "        pickled_file = self.conn.get(vkey)\n",
    "        return pickled_file\n",
    "    \n",
    "    \"\"\"This will \"\"\"\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        print(\"Reset all syncing information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisStorage(RedisFileProcessor):\n",
    "    def __init__(self):\n",
    "        self.versioning = RedisVersioning()\n",
    "        self.checksums = RedisChecksum()\n",
    "        self.sync = RedisSynchronizer()\n",
    "        self._connection = None\n",
    "        self._pipe = None\n",
    "        \n",
    "        # Current Placeholder Values\n",
    "        self.current_query = None\n",
    "        self.current_hash = None\n",
    "        self.current_pickle = None\n",
    "        self.current_checksum = None\n",
    "    \n",
    "    @property\n",
    "    def connection(self) -> Redis:\n",
    "        if self._connection is None:\n",
    "            raise AttributeError(\"Missing a redis connection\")\n",
    "        return self._connection\n",
    "    \n",
    "    @connection.setter\n",
    "    def connection(self, _connection):\n",
    "        self._connection = _connection\n",
    "    \n",
    "    @property\n",
    "    def query(self):\n",
    "        if self.current_query is None:\n",
    "            raise AttributeError(\"Current Query Doesn't Exist\")\n",
    "        return self.current_query\n",
    "    \n",
    "    @property\n",
    "    def chash(self):\n",
    "        if self.current_hash is None:\n",
    "            self.current_hash = consistent_hash(self.query)\n",
    "        return self.current_hash\n",
    "    \n",
    "    def process(self, obj:Any):\n",
    "        \"\"\"Completely process and object so you can use it elsewhere\"\"\"\n",
    "        \n",
    "        self.current_pickle = serialize(obj)\n",
    "        return self.current_pickle\n",
    "    \n",
    "    def save(self, query:dict, obj:Any, is_version_update=True):\n",
    "        \"\"\" Save object at query \"\"\"\n",
    "        self.reset()\n",
    "        self.current_query = query\n",
    "        self.process(obj)\n",
    "        self.load_subs()\n",
    "        prior_existing = self.versioning.exist\n",
    "        \n",
    "        # If we're updating the version\n",
    "        if is_version_update and prior_existing:\n",
    "            updated = self.versioning.updated\n",
    "            self.sync.add(self.chash, updated, self.current_pickle)\n",
    "            logger.info(f\"Version: {updated}\")\n",
    "            return self\n",
    "        \n",
    "        version = self.versioning.version\n",
    "        logger.info(f\"Version: {version}\")\n",
    "        self.sync.add(self.chash, version, self.current_pickle)\n",
    "        return self\n",
    "    \n",
    "    def load(self, query:dict):\n",
    "        \"\"\" Load the latest model \"\"\"\n",
    "        \n",
    "        self.reset()\n",
    "        self.current_query = query\n",
    "        self.load_subs()\n",
    "        prior_existing = self.versioning.exist\n",
    "#         logger.info(prior_existing)\n",
    "        if prior_existing == False:\n",
    "            return None\n",
    "        version = self.versioning.version\n",
    "        _file = self.sync.load(self.chash, version)\n",
    "        n_file = deserialize(_file)\n",
    "        return n_file\n",
    "        \n",
    "        \n",
    "    def load_subs(self):\n",
    "        \"\"\"\n",
    "            # Load Subs\n",
    "            Load relavent information into the subclasses. \n",
    "            This is so we can check for the integrity of the checksums and\n",
    "        \"\"\"\n",
    "#         self.checksums.checksum = self.current_checksum\n",
    "        self.versioning.qhash = self.chash\n",
    "        \n",
    "    \n",
    "    def reset_placeholder_values(self):\n",
    "        self.current_query = None\n",
    "        self.current_hash = None\n",
    "    \n",
    "    def reset_subclasses(self):\n",
    "        self.versioning.reset()\n",
    "        self.checksums.reset()\n",
    "        self.sync.reset()\n",
    "    \n",
    "    def reset_pipes(self):\n",
    "        self.versioning.conn = self.connection\n",
    "        self.checksums.conn = self.connection\n",
    "        self.sync.conn = self.connection        \n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Set a new pipe. Reset all of the other variables \"\"\"\n",
    "        super().reset()\n",
    "        self.reset_placeholder_values()\n",
    "        self.reset_subclasses()\n",
    "        self.reset_pipes()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "rstore = RedisStorage()\n",
    "rstore.connection = redconnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleObject:\n",
    "    def __init__(self):\n",
    "        self.one = \"two\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobj = SampleObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle1 = dill.dumps(sobj, ) # sobj.__reduce__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sobj2  = SampleObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle2 = dill.dumps(sobj2) # sobj.__reduce__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle1 == pickle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    cpickle1 = cloudpickle.dumps(sobj)\\n    cpickle2 = cloudpickle.dumps(sobj2)\\n    compressed1 = lz4.frame.compress(cpickle1)\\n    compressed2 = lz4.frame.compress(cpickle2)\\n    csum1 = create_checksum(compressed1)\\n    csum2 = create_checksum(compressed2)\\n'"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cpickle1 = cloudpickle.dumps(sobj)\n",
    "    cpickle2 = cloudpickle.dumps(sobj2)\n",
    "    compressed1 = lz4.frame.compress(cpickle1)\n",
    "    compressed2 = lz4.frame.compress(cpickle2)\n",
    "    csum1 = create_checksum(compressed1)\n",
    "    csum2 = create_checksum(compressed2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csum1 == csum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_digest = hmac.new('shared-key', pickled_data, hashlib.sha1).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobj3  = SampleObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting versioning variables\n",
      "Reset all syncing information\n",
      "Resetting versioning variables\n",
      "Reset all syncing information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-23 22:30:12.562 | INFO     | __main__:save:59 - Version: 0.0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting versioning variables\n",
      "Reset all syncing information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SampleObject at 0x7f710e1bad68>"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rstore.reset()\n",
    ".save({'one': 'twosss'}, sobj3, is_version_update=False).load({'one': 'twosss'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redconnection.zadd(f\"{item}:version_set\", {\"0.0.1\": maya.now()._epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'0.0.1']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redconnection.zrange(f\"{item}:version_set\", -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
